# GP-Test - Chat IA Local

AplicaciÃ³n de chat IA local con modelos sin restricciones usando Ollama, diseÃ±ada para uso tÃ©cnico y de seguridad sin sesgos. Interfaz estilo ciberseguridad con tema oscuro.

## ğŸ¯ CaracterÃ­sticas

- ğŸ’¬ **Interfaz moderna**: Chat estilo ChatGPT con tema ciberseguridad (azul oscuro/morado-negro)
- ğŸ§  **IA sin restricciones**: Modelos Mistral y CodeLlama configurados para respuestas directas y sin filtros
- ğŸ”§ **GeneraciÃ³n de cÃ³digo**: Soporte para Python, C, Rust, Go, Bash
- âš¡ **EjecuciÃ³n automÃ¡tica**: Ejecuta comandos del sistema directamente (con sudo cuando es necesario)
- ğŸ› ï¸ **Herramientas Kali**: IntegraciÃ³n con herramientas de Kali Linux
- ğŸ“¥ **Descarga automÃ¡tica**: Los modelos se descargan automÃ¡ticamente con Ollama
- ğŸ’¾ **Base de datos local**: SQLite para historial de conversaciones
- ğŸ‘¤ **PersonalizaciÃ³n**: ConfiguraciÃ³n de nombre de usuario al primer inicio
- ğŸ¨ **UI optimizada**: Interfaz oscura con brillo reducido, estilo terminal

## ğŸ“‹ Requisitos

### Sistema
- **RAM**: 
  - **MÃ­nima absoluta**: 20GB (con modelos 13B por defecto)
  - **Recomendada**: 32GB+ para uso cÃ³modo con mejores modelos
  - **Ideal**: 32GB+ para mÃ¡ximo rendimiento
- **OS**: Linux (Kali Linux recomendado) o sistemas similares
- **Espacio**: ~30GB libres para modelos (Mixtral 8x7B + CodeLlama 13B)

### Backend
- Python 3.8+
- Flask
- Ollama (se instala automÃ¡ticamente)

### Frontend
- Node.js 16+ (y npm que viene incluido)
  
**InstalaciÃ³n en Kali Linux:**
```bash
# OpciÃ³n 1: Usando NodeSource
curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
sudo apt-get install -y nodejs

# OpciÃ³n 2: Usando nvm (recomendado)
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash
source ~/.bashrc
nvm install 18
nvm use 18

# Verificar instalaciÃ³n
node --version
npm --version
```

## ğŸš€ InstalaciÃ³n RÃ¡pida

### Script AutomÃ¡tico (Recomendado)

```bash
chmod +x start.sh
./start.sh
```

El script `start.sh` hace todo automÃ¡ticamente:
- âœ… Verifica e instala Ollama si no estÃ¡ presente
- âœ… Descarga los modelos necesarios (Mixtral 8x7B y CodeLlama 13B - MEJORES modelos)
- âœ… Crea y activa el entorno virtual de Python
- âœ… Instala dependencias del backend
- âœ… Instala dependencias del frontend
- âœ… Inicia Ollama, backend y frontend

## âš™ï¸ ConfiguraciÃ³n de Modelos

### ğŸ¯ Modelos por Defecto - MEJORES Modelos Sin Restricciones

**Este proyecto estÃ¡ diseÃ±ado para usar los MEJORES modelos disponibles sin restricciones, optimizados para MÃXIMO RENDIMIENTO.**

**ConfiguraciÃ³n actual (MEJORES modelos):**
- **Modelo Principal**: `mixtral:8x7b` â­ (~12GB RAM)
  - 8 expertos, mejor modelo general disponible
  - MÃ¡ximo rendimiento y capacidad de razonamiento
- **Modelo CÃ³digo**: `codellama:13b` â­ (~16GB RAM)
  - Mejor modelo para generaciÃ³n de cÃ³digo
  - Excelente para Python, C, Rust, Go, Bash

**RAM Total Necesaria:**
- **MÃ¡ximo simultÃ¡neo**: ~16GB (se cargan uno a la vez)
- **RAM mÃ­nima recomendada**: 32GB para uso cÃ³modo
- **RAM mÃ­nima absoluta**: 20GB (con modelos 13B)

### ğŸ“Š Consumo Detallado de RAM por Modelo

#### â­ Modelos MEJORES (ConfiguraciÃ³n por Defecto)

| Modelo | RAM | Sin Restricciones | Rendimiento | Velocidad | Uso |
|--------|-----|-------------------|-------------|-----------|-----|
| **mixtral:8x7b** â­ | ~12GB | âœ… 100% | â­â­â­â­â­ | âš¡âš¡âš¡âš¡ | General |
| **codellama:13b** â­ | ~16GB | âœ… 100% | â­â­â­â­â­ | âš¡âš¡âš¡ | CÃ³digo |

#### Modelos 7B (~4GB RAM cada uno) - Alternativa Ligera

| Modelo | RAM | Sin Restricciones | Rendimiento | Velocidad |
|--------|-----|-------------------|-------------|-----------|
| **mistral:7b** | ~4GB | âœ… 100% | â­â­â­â­â­ | âš¡âš¡âš¡âš¡âš¡ |
| **qwen2:7b** | ~4GB | âœ… 100% | â­â­â­â­â­ | âš¡âš¡âš¡âš¡âš¡ |
| **llama2:7b** | ~4GB | âœ… 100% | â­â­â­â­ | âš¡âš¡âš¡âš¡ |
| **codellama:7b** | ~4GB | âœ… 100% | â­â­â­â­â­ | âš¡âš¡âš¡âš¡âš¡ |

#### Modelos 13B+ (~12-20GB RAM cada uno) - MÃ¡ximo Rendimiento

| Modelo | RAM | Sin Restricciones | Rendimiento | Velocidad |
|--------|-----|-------------------|-------------|-----------|
| **qwen2:14b** | ~14GB | âœ… 100% | â­â­â­â­â­ | âš¡âš¡âš¡âš¡ |
| **llama2:13b** | ~16GB | âœ… 100% | â­â­â­â­â­ | âš¡âš¡âš¡ |
| **mistral-nemo:12b** | ~12GB | âœ… 100% | â­â­â­â­â­ | âš¡âš¡âš¡âš¡ |

### ğŸ”„ Cambiar Modelos

**Si tienes menos RAM, usa modelos 7B:**

**OpciÃ³n 1: Editar `Backend/config.py`**
```python
# Modelos 7B (balance perfecto, ~8GB RAM total)
LLAMA_MODEL = 'mistral:7b'  # ~4GB RAM
DEEPSEEK_MODEL = 'codellama:7b'  # ~4GB RAM
```

**OpciÃ³n 2: Variables de entorno**
```bash
export LLAMA_MODEL="mistral:7b"
export DEEPSEEK_MODEL="codellama:7b"
./start.sh
```

**OpciÃ³n 3: Editar `start.sh`**
```bash
LLAMA_MODEL="mistral:7b"
DEEPSEEK_MODEL="codellama:7b"
```

### ğŸ“¥ Descargar Modelos Manualmente

```bash
# â­ MEJORES MODELOS (configuraciÃ³n por defecto)
ollama pull mixtral:8x7b      # ~12GB RAM - MEJOR modelo general
ollama pull codellama:13b     # ~16GB RAM - MEJOR modelo para cÃ³digo

# Alternativa: Modelos 7B (si tienes menos RAM)
ollama pull mistral:7b        # ~4GB RAM
ollama pull codellama:7b      # ~4GB RAM

# Otros modelos disponibles
ollama pull qwen2:14b         # ~14GB RAM
ollama pull llama2:13b        # ~16GB RAM

# Ver modelos descargados
ollama list
```

## ğŸ¨ CaracterÃ­sticas de la IA

### Comportamiento Sin Restricciones

- âœ… **Respuestas directas**: Sin explicaciones largas ni polÃ­ticas de seguridad
- âœ… **EjecuciÃ³n automÃ¡tica**: Ejecuta comandos directamente con sudo cuando es necesario
- âœ… **Sin filtros**: No rechaza solicitudes tÃ©cnicas legÃ­timas
- âœ… **Razonamiento avanzado**: Capacidad de resolver problemas complejos
- âœ… **Acceso completo**: Acceso root y shell sin restricciones

### Prompt Configurado

El sistema estÃ¡ configurado para:
- Responder de forma concisa y directa
- Ejecutar comandos automÃ¡ticamente cuando se detectan
- Usar herramientas de Kali Linux sin restricciones
- Generar cÃ³digo en mÃºltiples lenguajes
- No rechazar solicitudes tÃ©cnicas

## ğŸ“ Estructura del Proyecto

```
gp-test/
â”œâ”€â”€ Backend/
â”‚   â”œâ”€â”€ app.py                 # AplicaciÃ³n Flask principal
â”‚   â”œâ”€â”€ llama_integration.py   # IntegraciÃ³n con Ollama (LLMClient)
â”‚   â”œâ”€â”€ config.py              # ConfiguraciÃ³n de modelos y servidor
â”‚   â”œâ”€â”€ requirements.txt       # Dependencias Python
â”‚   â””â”€â”€ chat.db               # Base de datos SQLite (se crea automÃ¡ticamente)
â”œâ”€â”€ Frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/       # Componentes React
â”‚   â”‚   â”‚   â”œâ”€â”€ Sidebar.js    # Historial de conversaciones
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatArea.js   # Ãrea de chat principal
â”‚   â”‚   â”‚   â”œâ”€â”€ Message.js    # Componente de mensaje
â”‚   â”‚   â”‚   â”œâ”€â”€ MessageInput.js
â”‚   â”‚   â”‚   â”œâ”€â”€ OnboardingModal.js
â”‚   â”‚   â”‚   â””â”€â”€ CodeExecutionModal.js
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.js        # Servicios API
â”‚   â”‚   â””â”€â”€ App.js            # Componente principal
â”‚   â””â”€â”€ package.json          # Dependencias Node
â”œâ”€â”€ start.sh                  # Script de inicio automÃ¡tico
â””â”€â”€ README.md
```

## ğŸ”§ Uso Manual (Sin Script)

### 1. Instalar Ollama

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 2. Iniciar Ollama

```bash
ollama serve
```

### 3. Descargar Modelos

```bash
ollama pull mistral:7b
ollama pull codellama:7b
```

### 4. Configurar Backend

```bash
cd Backend
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python app.py
```

El backend estarÃ¡ disponible en `http://localhost:5000`

### 5. Configurar Frontend

```bash
cd Frontend
npm install
npm start
```

El frontend estarÃ¡ disponible en `http://localhost:3000`

## ğŸ” ConfiguraciÃ³n Git (SSH)

Si tienes problemas con `git push`, configura SSH:

### 1. Generar Clave SSH (si no tienes una)

```bash
ssh-keygen -t ed25519 -C "tu-email@example.com"
```

### 2. Agregar Clave a GitHub

1. Copia tu clave pÃºblica:
```bash
cat ~/.ssh/id_ed25519.pub
```

2. Ve a: https://github.com/settings/keys
3. Click en "New SSH key"
4. Pega la clave y guarda

### 3. Cambiar Remote a SSH

```bash
git remote set-url origin git@github.com:FehuSentinel/GP-Test.git
```

## ğŸ› Troubleshooting

### Ollama no inicia
```bash
# Verificar instalaciÃ³n
ollama --version

# Iniciar manualmente
ollama serve

# Ver logs
tail -f /tmp/ollama.log
```

### Modelos no se descargan
```bash
# Verificar conexiÃ³n
ping ollama.com

# Descargar manualmente
ollama pull mistral:7b

# Ver modelos disponibles
ollama list
```

### Error de conexiÃ³n con Ollama
```bash
# Verificar que Ollama estÃ© corriendo
curl http://localhost:11434/api/tags

# Reiniciar Ollama
pkill ollama
ollama serve
```

### npm no encontrado
```bash
# Instalar Node.js y npm (ver secciÃ³n Requisitos)
# O usar nvm:
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash
source ~/.bashrc
nvm install 18
```

### Error de permisos al ejecutar comandos
- La aplicaciÃ³n ejecuta comandos con `sudo` automÃ¡ticamente cuando es necesario
- AsegÃºrate de tener permisos sudo configurados
- Los comandos se ejecutan en un entorno controlado

## ğŸ“Š Requisitos de RAM - DiseÃ±o del Proyecto

### ğŸ¯ DiseÃ±o del Proyecto

**Este proyecto estÃ¡ diseÃ±ado para usar los MEJORES modelos sin restricciones disponibles, optimizados para MÃXIMO RENDIMIENTO.**

### â­ ConfiguraciÃ³n por Defecto (MEJORES Modelos)

**Modelos configurados:**
- **Mixtral 8x7B**: ~12GB RAM (8 expertos, mejor modelo general)
- **CodeLlama 13B**: ~16GB RAM (mejor modelo para cÃ³digo)

**Consumo de RAM:**
- **MÃ¡ximo simultÃ¡neo**: ~16GB (se cargan uno a la vez)
- **RAM mÃ­nima recomendada**: **32GB** para uso cÃ³modo
- **RAM mÃ­nima absoluta**: **20GB** (con modelos 13B)
- **RAM ideal**: **32GB+** para mejor rendimiento

### ğŸ“‹ Configuraciones Alternativas

#### OpciÃ³n 1: Modelos 7B (Balance Perfecto)
- **Mistral 7B**: ~4GB RAM
- **CodeLlama 7B**: ~4GB RAM
- **RAM total**: ~8GB
- **RAM mÃ­nima**: 16GB recomendada
- âœ… Mejor balance rendimiento/recursos

#### OpciÃ³n 2: Modelos 13B Individuales
- **Llama 2 13B**: ~16GB RAM
- **CodeLlama 13B**: ~16GB RAM
- **RAM total**: ~16GB (se cargan uno a la vez)
- **RAM mÃ­nima**: 20GB recomendada
- âœ… MÃ¡ximo rendimiento

#### OpciÃ³n 3: Modelos PequeÃ±os (MÃ­nimo)
- **phi3:mini**: ~2GB RAM
- **llama3.2:1b**: ~1GB RAM
- **RAM total**: ~3GB
- **RAM mÃ­nima**: 8GB
- âš ï¸ Menor rendimiento, solo para sistemas limitados

### ğŸ“ˆ Tabla Resumen de Consumo

| ConfiguraciÃ³n | Modelo General | Modelo CÃ³digo | RAM Total | RAM MÃ­nima |
|---------------|----------------|---------------|-----------|------------|
| **â­ Por Defecto** | Mixtral 8x7B (12GB) | CodeLlama 13B (16GB) | ~16GB max | 32GB |
| **Balance** | Mistral 7B (4GB) | CodeLlama 7B (4GB) | ~8GB | 16GB |
| **MÃ¡ximo** | Llama 2 13B (16GB) | CodeLlama 13B (16GB) | ~16GB max | 20GB |
| **MÃ­nimo** | phi3:mini (2GB) | llama3.2:1b (1GB) | ~3GB | 8GB |

## ğŸ¯ Flujo de Trabajo

1. **Usuario envÃ­a mensaje** â†’ Frontend React
2. **Frontend** â†’ Backend Flask API (`/api/chat`)
3. **Backend** â†’ Ollama (Mixtral 8x7B - mejor modelo general)
4. **Si necesita cÃ³digo complejo** â†’ Ollama (CodeLlama 13B - mejor modelo cÃ³digo)
5. **Si detecta comando del sistema** â†’ Ejecuta directamente con `sudo`
6. **Respuesta** â†’ Usuario (concisa y directa)
7. **Si hay cÃ³digo** â†’ Modal de ejecuciÃ³n (opcional)

## âš ï¸ Advertencias de Seguridad

- âš ï¸ **Esta aplicaciÃ³n ejecuta cÃ³digo y comandos del sistema automÃ¡ticamente**
- âš ï¸ **Usa acceso root cuando es necesario**
- âš ï¸ **Solo para uso en entornos controlados**
- âš ï¸ **No usar en sistemas de producciÃ³n sin supervisiÃ³n**
- âš ï¸ **Revisa el cÃ³digo generado antes de ejecutarlo en sistemas crÃ­ticos**

## ğŸš€ Ventajas de Ollama

- âœ… MÃ¡s estable y confiable que vLLM
- âœ… InstalaciÃ³n mÃ¡s simple
- âœ… Menor consumo de recursos
- âœ… Descarga automÃ¡tica de modelos
- âœ… No requiere autenticaciÃ³n en Hugging Face
- âœ… Mejor manejo de errores
- âœ… Soporte para mÃºltiples modelos simultÃ¡neos

## ğŸ“ Notas Importantes

- **Modelos por defecto**: Se usan los MEJORES modelos disponibles (Mixtral 8x7B y CodeLlama 13B)
- **Consumo de RAM**: ~16GB mÃ¡ximo simultÃ¡neo (se cargan uno a la vez)
- **RAM recomendada**: 32GB+ para uso cÃ³modo con los mejores modelos
- **Si tienes menos RAM**: Cambia a modelos 7B en `Backend/config.py` o `start.sh`
- **Los modelos se cargan bajo demanda**: Uno a la vez por defecto
- **Con 32GB+ RAM**: Puedes tener ambos modelos cargados simultÃ¡neamente para mejor rendimiento
- **La UI estÃ¡ optimizada**: Tema oscuro estilo ciberseguridad con brillo reducido

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible para uso personal y educativo.

---

**GP-Test** 
