# Chat IA Local - Sin Sesgo

AplicaciÃ³n de chat IA local con Llama y DeepSeek usando vLLM, diseÃ±ada para uso tÃ©cnico y de seguridad sin sesgos.

## CaracterÃ­sticas

- ğŸ’¬ Interfaz de chat moderna estilo ChatGPT
- ğŸ§  IntegraciÃ³n con Llama local (vÃ­a vLLM)
- ğŸ¤– IntegraciÃ³n con DeepSeek local (vÃ­a vLLM) para generaciÃ³n de cÃ³digo
- ğŸ”§ GeneraciÃ³n y ejecuciÃ³n de scripts (Python, C, Rust, Go, Bash)
- ğŸ“¥ Descarga automÃ¡tica de modelos si no estÃ¡n disponibles
- ğŸ’¾ Base de datos SQLite local
- ğŸ¯ Prompt sin sesgo configurado
- ğŸ‘¤ PersonalizaciÃ³n con nombre de usuario
- ğŸ› ï¸ Uso de herramientas de Kali Linux

## Requisitos

### Backend
- Python 3.8+
- Flask
- vLLM instalado
- Cuenta de Hugging Face (para descargar modelos)

### Frontend
- Node.js 16+ (y npm que viene incluido)
  
  **InstalaciÃ³n en Kali Linux:**
  ```bash
  # OpciÃ³n 1: Usando NodeSource
  curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
  sudo apt-get install -y nodejs
  
  # OpciÃ³n 2: Usando nvm (recomendado)
  curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash
  source ~/.bashrc
  nvm install 18
  nvm use 18
  
  # Verificar instalaciÃ³n
  node --version
  npm --version
  ```

## InstalaciÃ³n

### 1. Instalar vLLM

```bash
pip install vllm
```

### 2. Autenticarse en Hugging Face

```bash
huggingface-cli login
```

### 3. Instalar dependencias del Backend

```bash
cd Backend
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 4. Instalar dependencias del Frontend

```bash
cd Frontend
npm install
```

## ConfiguraciÃ³n

### Modelos

Los modelos se descargan automÃ¡ticamente la primera vez que se usan. Los modelos por defecto son:

- **Llama**: `meta-llama/Llama-3.1-8B-Instruct`
- **DeepSeek**: `deepseek-ai/deepseek-coder-6.7b-instruct`

Puedes cambiarlos en `Backend/config.py` o mediante variables de entorno.

### Frontend

Crea un archivo `.env` en la carpeta Frontend (opcional):
```
REACT_APP_API_URL=http://localhost:5000/api
```

## Uso

### OpciÃ³n 1: Script automÃ¡tico (Recomendado)

```bash
./start.sh
```

Este script:
- Verifica si vLLM estÃ¡ corriendo
- Instala dependencias automÃ¡ticamente
- Inicia backend y frontend

### OpciÃ³n 2: Manual

#### 1. Iniciar vLLM

```bash
# Con Llama (para chat general)
vllm serve meta-llama/Llama-3.1-8B-Instruct

# O con DeepSeek (para generaciÃ³n de cÃ³digo)
vllm serve deepseek-ai/deepseek-coder-6.7b-instruct
```

**Nota**: vLLM solo puede cargar un modelo a la vez. Para cambiar de modelo, detÃ©n vLLM e inÃ­cialo con el otro modelo.

#### 2. Verificar/Configurar modelos

```bash
cd Backend
python3 setup_models.py
```

#### 3. Iniciar Backend

```bash
cd Backend
source venv/bin/activate
python app.py
```

El backend estarÃ¡ disponible en `http://localhost:5000`

#### 4. Iniciar Frontend

```bash
cd Frontend
npm start
```

El frontend estarÃ¡ disponible en `http://localhost:3000`

## Estructura del Proyecto

```
gp-test/
â”œâ”€â”€ Backend/
â”‚   â”œâ”€â”€ app.py                 # AplicaciÃ³n Flask principal
â”‚   â”œâ”€â”€ llama_integration.py   # IntegraciÃ³n con Llama/DeepSeek vÃ­a vLLM
â”‚   â”œâ”€â”€ setup_models.py        # Script para verificar/descargar modelos
â”‚   â”œâ”€â”€ config.py              # ConfiguraciÃ³n
â”‚   â”œâ”€â”€ requirements.txt       # Dependencias Python
â”‚   â””â”€â”€ chat.db               # Base de datos SQLite (se crea automÃ¡ticamente)
â”œâ”€â”€ Frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/       # Componentes React
â”‚   â”‚   â”œâ”€â”€ services/         # Servicios API
â”‚   â”‚   â””â”€â”€ App.js            # Componente principal
â”‚   â””â”€â”€ package.json          # Dependencias Node
â”œâ”€â”€ start.sh                  # Script de inicio automÃ¡tico
â””â”€â”€ README.md
```

## CaracterÃ­sticas TÃ©cnicas

- **Sin sesgo**: El prompt estÃ¡ configurado para ser objetivo y tÃ©cnico
- **EjecuciÃ³n segura**: Los scripts se muestran antes de ejecutarse
- **Historial persistente**: Todas las conversaciones se guardan en SQLite
- **CÃ³digo generado**: Soporte para mÃºltiples lenguajes de programaciÃ³n
- **IntegraciÃ³n DeepSeek**: Llama puede solicitar cÃ³digo complejo a DeepSeek cuando sea necesario
- **100% Local**: Todo funciona localmente sin APIs externas

## Flujo de Trabajo

1. Usuario envÃ­a mensaje â†’ Frontend
2. Frontend â†’ Backend Flask API
3. Backend â†’ Llama (vÃ­a vLLM)
4. Si Llama necesita cÃ³digo complejo â†’ DeepSeek (vÃ­a vLLM)
5. Respuesta â†’ Usuario
6. Si hay cÃ³digo â†’ Usuario decide si ejecutarlo

## Notas de Seguridad

âš ï¸ **ADVERTENCIA**: Esta aplicaciÃ³n puede ejecutar cÃ³digo y comandos del sistema. Ãšsala con precauciÃ³n y solo en entornos controlados.

## Troubleshooting

### vLLM no inicia
- Verifica que tengas suficiente RAM (recomendado: 16GB+)
- AsegÃºrate de estar autenticado en Hugging Face: `huggingface-cli login`
- Verifica que el modelo existe y tienes acceso

### Modelos no se descargan
- Verifica tu conexiÃ³n a internet
- AsegÃºrate de estar autenticado en Hugging Face
- Algunos modelos requieren solicitar acceso en Hugging Face

### Error de conexiÃ³n
- Verifica que vLLM estÃ© corriendo en `http://localhost:8000`
- Verifica que el modelo estÃ© cargado correctamente

## Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible para uso personal y educativo.
