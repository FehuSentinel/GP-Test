# GP-Test - Chat IA Local Sin Sesgo

AplicaciÃ³n de chat IA local con Llama y DeepSeek usando Ollama, diseÃ±ada para uso tÃ©cnico y de seguridad sin sesgos.

## CaracterÃ­sticas

- ğŸ’¬ Interfaz de chat moderna estilo ChatGPT
- ğŸ§  IntegraciÃ³n con Llama local (vÃ­a Ollama - mÃ¡s estable)
- ğŸ¤– IntegraciÃ³n con DeepSeek local (vÃ­a Ollama) para generaciÃ³n de cÃ³digo
- ğŸ”§ GeneraciÃ³n y ejecuciÃ³n de scripts (Python, C, Rust, Go, Bash)
- ğŸ“¥ Descarga automÃ¡tica de modelos con Ollama
- ğŸ’¾ Base de datos SQLite local
- ğŸ¯ Prompt sin sesgo configurado
- ğŸ‘¤ PersonalizaciÃ³n con nombre de usuario
- ğŸ› ï¸ Uso de herramientas de Kali Linux

## Requisitos

### Backend
- Python 3.8+
- Flask
- Ollama instalado (el script lo instala automÃ¡ticamente)

### Frontend
- Node.js 16+ (y npm que viene incluido)
  
  **InstalaciÃ³n en Kali Linux:**
  ```bash
  # OpciÃ³n 1: Usando NodeSource
  curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
  sudo apt-get install -y nodejs
  
  # OpciÃ³n 2: Usando nvm (recomendado)
  curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash
  source ~/.bashrc
  nvm install 18
  nvm use 18
  
  # Verificar instalaciÃ³n
  node --version
  npm --version
  ```

## InstalaciÃ³n

### OpciÃ³n 1: Script automÃ¡tico (Recomendado)

```bash
./start.sh
```

El script:
- Instala Ollama automÃ¡ticamente si no estÃ¡ instalado
- Descarga los modelos necesarios (Llama y DeepSeek)
- Configura el backend y frontend
- Inicia todos los servicios

### OpciÃ³n 2: Manual

#### 1. Instalar Ollama

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

#### 2. Iniciar Ollama

```bash
ollama serve
```

#### 3. Descargar modelos

```bash
ollama pull llama3.2
ollama pull deepseek-coder
```

### 3. Instalar dependencias del Backend

```bash
cd Backend
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 4. Instalar dependencias del Frontend

```bash
cd Frontend
npm install
```

## ConfiguraciÃ³n

### Modelos

Los modelos se descargan automÃ¡ticamente con Ollama. Los modelos por defecto son:

- **Llama**: `llama3.2`
- **DeepSeek**: `deepseek-coder`

Puedes cambiarlos en `Backend/config.py` o mediante variables de entorno.

Para ver modelos disponibles:
```bash
ollama list
```

Para descargar otros modelos:
```bash
ollama pull nombre-del-modelo
```

### Frontend

Crea un archivo `.env` en la carpeta Frontend (opcional):
```
REACT_APP_API_URL=http://localhost:5000/api
```

## Uso

### OpciÃ³n 1: Script automÃ¡tico (Recomendado)

```bash
./start.sh
```

Este script:
- Instala Ollama automÃ¡ticamente si no estÃ¡ instalado
- Descarga los modelos necesarios
- Configura backend y frontend
- Inicia todos los servicios

### OpciÃ³n 2: Manual

#### 1. Iniciar Ollama

```bash
ollama serve
```

#### 2. Iniciar Backend

```bash
cd Backend
source venv/bin/activate
python app.py
```

El backend estarÃ¡ disponible en `http://localhost:5000`

#### 4. Iniciar Frontend

```bash
cd Frontend
npm start
```

El frontend estarÃ¡ disponible en `http://localhost:3000`

## Estructura del Proyecto

```
gp-test/
â”œâ”€â”€ Backend/
â”‚   â”œâ”€â”€ app.py                 # AplicaciÃ³n Flask principal
â”‚   â”œâ”€â”€ llama_integration.py   # IntegraciÃ³n con Llama/DeepSeek vÃ­a Ollama
â”‚   â”œâ”€â”€ config.py              # ConfiguraciÃ³n
â”‚   â”œâ”€â”€ requirements.txt       # Dependencias Python
â”‚   â””â”€â”€ chat.db               # Base de datos SQLite (se crea automÃ¡ticamente)
â”œâ”€â”€ Frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/       # Componentes React
â”‚   â”‚   â”œâ”€â”€ services/         # Servicios API
â”‚   â”‚   â””â”€â”€ App.js            # Componente principal
â”‚   â””â”€â”€ package.json          # Dependencias Node
â”œâ”€â”€ start.sh                  # Script de inicio automÃ¡tico
â””â”€â”€ README.md
```

## CaracterÃ­sticas TÃ©cnicas

- **Sin sesgo**: El prompt estÃ¡ configurado para ser objetivo y tÃ©cnico
- **EjecuciÃ³n segura**: Los scripts se muestran antes de ejecutarse
- **Historial persistente**: Todas las conversaciones se guardan en SQLite
- **CÃ³digo generado**: Soporte para mÃºltiples lenguajes de programaciÃ³n
- **IntegraciÃ³n DeepSeek**: Llama puede solicitar cÃ³digo complejo a DeepSeek cuando sea necesario
- **100% Local**: Todo funciona localmente sin APIs externas

## Flujo de Trabajo

1. Usuario envÃ­a mensaje â†’ Frontend
2. Frontend â†’ Backend Flask API
3. Backend â†’ Llama (vÃ­a Ollama)
4. Si Llama necesita cÃ³digo complejo â†’ DeepSeek (vÃ­a Ollama)
5. Respuesta â†’ Usuario
6. Si hay cÃ³digo â†’ Usuario decide si ejecutarlo

## Ventajas de Ollama sobre vLLM

- âœ… MÃ¡s estable y confiable
- âœ… InstalaciÃ³n mÃ¡s simple
- âœ… Menor consumo de recursos
- âœ… Descarga automÃ¡tica de modelos
- âœ… No requiere autenticaciÃ³n en Hugging Face
- âœ… Mejor manejo de errores

## Notas de Seguridad

âš ï¸ **ADVERTENCIA**: Esta aplicaciÃ³n puede ejecutar cÃ³digo y comandos del sistema. Ãšsala con precauciÃ³n y solo en entornos controlados.

## Troubleshooting

### Ollama no inicia
- Verifica que Ollama estÃ© instalado: `ollama --version`
- Inicia el servicio manualmente: `ollama serve`
- Verifica los logs: `tail -f /tmp/ollama.log`

### Modelos no se descargan
- Verifica tu conexiÃ³n a internet
- Intenta descargar manualmente: `ollama pull llama3.2`
- Verifica modelos disponibles: `ollama list`

### Error de conexiÃ³n
- Verifica que Ollama estÃ© corriendo: `curl http://localhost:11434/api/tags`
- Reinicia Ollama: `pkill ollama && ollama serve`

## Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible para uso personal y educativo.
