# GP-Test - Chat IA Local Sin Sesgo

AplicaciÃ³n de chat IA local con modelos sin restricciones usando Ollama, diseÃ±ada para uso tÃ©cnico y de seguridad sin sesgos. Interfaz estilo ciberseguridad con tema oscuro.

## ğŸ¯ CaracterÃ­sticas

- ğŸ’¬ **Interfaz moderna**: Chat estilo ChatGPT con tema ciberseguridad (azul oscuro/morado-negro)
- ğŸ§  **IA sin restricciones**: Modelos Mistral y CodeLlama configurados para respuestas directas y sin filtros
- ğŸ”§ **GeneraciÃ³n de cÃ³digo**: Soporte para Python, C, Rust, Go, Bash
- âš¡ **EjecuciÃ³n automÃ¡tica**: Ejecuta comandos del sistema directamente (con sudo cuando es necesario)
- ğŸ› ï¸ **Herramientas Kali**: IntegraciÃ³n con herramientas de Kali Linux
- ğŸ“¥ **Descarga automÃ¡tica**: Los modelos se descargan automÃ¡ticamente con Ollama
- ğŸ’¾ **Base de datos local**: SQLite para historial de conversaciones
- ğŸ‘¤ **PersonalizaciÃ³n**: ConfiguraciÃ³n de nombre de usuario al primer inicio
- ğŸ¨ **UI optimizada**: Interfaz oscura con brillo reducido, estilo terminal

## ğŸ“‹ Requisitos

### Sistema
- **RAM**: MÃ­nimo 8GB (recomendado 16GB+, optimizado para 25GB)
- **OS**: Linux (Kali Linux recomendado) o sistemas similares
- **Espacio**: ~10GB libres para modelos

### Backend
- Python 3.8+
- Flask
- Ollama (se instala automÃ¡ticamente)

### Frontend
- Node.js 16+ (y npm que viene incluido)
  
**InstalaciÃ³n en Kali Linux:**
```bash
# OpciÃ³n 1: Usando NodeSource
curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
sudo apt-get install -y nodejs

# OpciÃ³n 2: Usando nvm (recomendado)
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash
source ~/.bashrc
nvm install 18
nvm use 18

# Verificar instalaciÃ³n
node --version
npm --version
```

## ğŸš€ InstalaciÃ³n RÃ¡pida

### Script AutomÃ¡tico (Recomendado)

```bash
chmod +x start.sh
./start.sh
```

El script `start.sh` hace todo automÃ¡ticamente:
- âœ… Verifica e instala Ollama si no estÃ¡ presente
- âœ… Descarga los modelos necesarios (Mistral 7B y CodeLlama 7B)
- âœ… Crea y activa el entorno virtual de Python
- âœ… Instala dependencias del backend
- âœ… Instala dependencias del frontend
- âœ… Inicia Ollama, backend y frontend

## âš™ï¸ ConfiguraciÃ³n de Modelos

### Modelos por Defecto (Optimizados para 25GB RAM)

**ConfiguraciÃ³n recomendada:**
- **Modelo Principal**: `mistral:7b` (~4GB RAM)
- **Modelo CÃ³digo**: `codellama:7b` (~4GB RAM)
- **RAM total**: ~5-6GB
- **RAM libre**: ~19GB disponibles

### Modelos Disponibles

#### Modelos 7B (~4GB RAM cada uno) - Balance Perfecto â­

| Modelo | Sin Restricciones | Rendimiento | Velocidad |
|--------|-------------------|-------------|-----------|
| **mistral:7b** | âœ… 100% | â­â­â­â­â­ | âš¡âš¡âš¡âš¡âš¡ |
| **qwen2:7b** | âœ… 100% | â­â­â­â­â­ | âš¡âš¡âš¡âš¡âš¡ |
| **llama2:7b** | âœ… 100% | â­â­â­â­ | âš¡âš¡âš¡âš¡ |
| **codellama:7b** | âœ… 100% | â­â­â­â­â­ | âš¡âš¡âš¡âš¡âš¡ |

#### Modelos 13B+ (~16-20GB RAM cada uno) - MÃ¡ximo Rendimiento

| Modelo | Sin Restricciones | Rendimiento | Velocidad |
|--------|-------------------|-------------|-----------|
| **mistral-nemo:12b** | âœ… 100% | â­â­â­â­â­ | âš¡âš¡âš¡âš¡ |
| **qwen2:14b** | âœ… 100% | â­â­â­â­â­ | âš¡âš¡âš¡âš¡ |
| **llama2:13b** | âœ… 100% | â­â­â­â­â­ | âš¡âš¡âš¡ |
| **codellama:13b** | âœ… 100% | â­â­â­â­â­ | âš¡âš¡âš¡ |

### Cambiar Modelos

**OpciÃ³n 1: Editar `Backend/config.py`**
```python
LLAMA_MODEL = 'mistral:7b'  # Cambia aquÃ­
DEEPSEEK_MODEL = 'codellama:7b'  # Cambia aquÃ­
```

**OpciÃ³n 2: Variables de entorno**
```bash
export LLAMA_MODEL="mistral-nemo:12b"
export DEEPSEEK_MODEL="codellama:13b"
./start.sh
```

**OpciÃ³n 3: Editar `start.sh`**
```bash
LLAMA_MODEL="mistral:7b"
DEEPSEEK_MODEL="codellama:7b"
```

### Descargar Modelos Manualmente

```bash
# Modelos 7B (recomendado)
ollama pull mistral:7b
ollama pull codellama:7b

# Modelos 13B (mÃ¡ximo rendimiento)
ollama pull mistral-nemo:12b
ollama pull codellama:13b

# Ver modelos descargados
ollama list
```

## ğŸ¨ CaracterÃ­sticas de la IA

### Comportamiento Sin Restricciones

- âœ… **Respuestas directas**: Sin explicaciones largas ni polÃ­ticas de seguridad
- âœ… **EjecuciÃ³n automÃ¡tica**: Ejecuta comandos directamente con sudo cuando es necesario
- âœ… **Sin filtros**: No rechaza solicitudes tÃ©cnicas legÃ­timas
- âœ… **Razonamiento avanzado**: Capacidad de resolver problemas complejos
- âœ… **Acceso completo**: Acceso root y shell sin restricciones

### Prompt Configurado

El sistema estÃ¡ configurado para:
- Responder de forma concisa y directa
- Ejecutar comandos automÃ¡ticamente cuando se detectan
- Usar herramientas de Kali Linux sin restricciones
- Generar cÃ³digo en mÃºltiples lenguajes
- No rechazar solicitudes tÃ©cnicas

## ğŸ“ Estructura del Proyecto

```
gp-test/
â”œâ”€â”€ Backend/
â”‚   â”œâ”€â”€ app.py                 # AplicaciÃ³n Flask principal
â”‚   â”œâ”€â”€ llama_integration.py   # IntegraciÃ³n con Ollama (LLMClient)
â”‚   â”œâ”€â”€ config.py              # ConfiguraciÃ³n de modelos y servidor
â”‚   â”œâ”€â”€ requirements.txt       # Dependencias Python
â”‚   â””â”€â”€ chat.db               # Base de datos SQLite (se crea automÃ¡ticamente)
â”œâ”€â”€ Frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/       # Componentes React
â”‚   â”‚   â”‚   â”œâ”€â”€ Sidebar.js    # Historial de conversaciones
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatArea.js   # Ãrea de chat principal
â”‚   â”‚   â”‚   â”œâ”€â”€ Message.js    # Componente de mensaje
â”‚   â”‚   â”‚   â”œâ”€â”€ MessageInput.js
â”‚   â”‚   â”‚   â”œâ”€â”€ OnboardingModal.js
â”‚   â”‚   â”‚   â””â”€â”€ CodeExecutionModal.js
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.js        # Servicios API
â”‚   â”‚   â””â”€â”€ App.js            # Componente principal
â”‚   â””â”€â”€ package.json          # Dependencias Node
â”œâ”€â”€ start.sh                  # Script de inicio automÃ¡tico
â””â”€â”€ README.md
```

## ğŸ”§ Uso Manual (Sin Script)

### 1. Instalar Ollama

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 2. Iniciar Ollama

```bash
ollama serve
```

### 3. Descargar Modelos

```bash
ollama pull mistral:7b
ollama pull codellama:7b
```

### 4. Configurar Backend

```bash
cd Backend
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python app.py
```

El backend estarÃ¡ disponible en `http://localhost:5000`

### 5. Configurar Frontend

```bash
cd Frontend
npm install
npm start
```

El frontend estarÃ¡ disponible en `http://localhost:3000`

## ğŸ” ConfiguraciÃ³n Git (SSH)

Si tienes problemas con `git push`, configura SSH:

### 1. Generar Clave SSH (si no tienes una)

```bash
ssh-keygen -t ed25519 -C "tu-email@example.com"
```

### 2. Agregar Clave a GitHub

1. Copia tu clave pÃºblica:
```bash
cat ~/.ssh/id_ed25519.pub
```

2. Ve a: https://github.com/settings/keys
3. Click en "New SSH key"
4. Pega la clave y guarda

### 3. Cambiar Remote a SSH

```bash
git remote set-url origin git@github.com:FehuSentinel/GP-Test.git
```

## ğŸ› Troubleshooting

### Ollama no inicia
```bash
# Verificar instalaciÃ³n
ollama --version

# Iniciar manualmente
ollama serve

# Ver logs
tail -f /tmp/ollama.log
```

### Modelos no se descargan
```bash
# Verificar conexiÃ³n
ping ollama.com

# Descargar manualmente
ollama pull mistral:7b

# Ver modelos disponibles
ollama list
```

### Error de conexiÃ³n con Ollama
```bash
# Verificar que Ollama estÃ© corriendo
curl http://localhost:11434/api/tags

# Reiniciar Ollama
pkill ollama
ollama serve
```

### npm no encontrado
```bash
# Instalar Node.js y npm (ver secciÃ³n Requisitos)
# O usar nvm:
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash
source ~/.bashrc
nvm install 18
```

### Error de permisos al ejecutar comandos
- La aplicaciÃ³n ejecuta comandos con `sudo` automÃ¡ticamente cuando es necesario
- AsegÃºrate de tener permisos sudo configurados
- Los comandos se ejecutan en un entorno controlado

## ğŸ“Š Requisitos de RAM

### ConfiguraciÃ³n Recomendada (25GB RAM)

**Modelos 7B:**
- RAM necesaria: ~5-6GB
- RAM libre: ~19GB
- âœ… Mejor balance rendimiento/recursos

**Modelos 13B:**
- RAM necesaria: ~16-20GB (se cargan uno a la vez)
- RAM libre: ~5-9GB
- âœ… MÃ¡ximo rendimiento

### ConfiguraciÃ³n MÃ­nima (8GB RAM)

**Modelos pequeÃ±os:**
- `phi3:mini` (~2GB RAM)
- `llama3.2:1b` (~1GB RAM)

## ğŸ¯ Flujo de Trabajo

1. **Usuario envÃ­a mensaje** â†’ Frontend React
2. **Frontend** â†’ Backend Flask API (`/api/chat`)
3. **Backend** â†’ Ollama (Mistral 7B)
4. **Si necesita cÃ³digo complejo** â†’ Ollama (CodeLlama 7B)
5. **Si detecta comando del sistema** â†’ Ejecuta directamente con `sudo`
6. **Respuesta** â†’ Usuario (concisa y directa)
7. **Si hay cÃ³digo** â†’ Modal de ejecuciÃ³n (opcional)

## âš ï¸ Advertencias de Seguridad

- âš ï¸ **Esta aplicaciÃ³n ejecuta cÃ³digo y comandos del sistema automÃ¡ticamente**
- âš ï¸ **Usa acceso root cuando es necesario**
- âš ï¸ **Solo para uso en entornos controlados**
- âš ï¸ **No usar en sistemas de producciÃ³n sin supervisiÃ³n**
- âš ï¸ **Revisa el cÃ³digo generado antes de ejecutarlo en sistemas crÃ­ticos**

## ğŸš€ Ventajas de Ollama

- âœ… MÃ¡s estable y confiable que vLLM
- âœ… InstalaciÃ³n mÃ¡s simple
- âœ… Menor consumo de recursos
- âœ… Descarga automÃ¡tica de modelos
- âœ… No requiere autenticaciÃ³n en Hugging Face
- âœ… Mejor manejo de errores
- âœ… Soporte para mÃºltiples modelos simultÃ¡neos

## ğŸ“ Notas

- Los modelos se cargan bajo demanda (uno a la vez por defecto)
- Con 25GB RAM puedes tener ambos modelos cargados simultÃ¡neamente
- Mistral 7B es la mejor opciÃ³n balance para la mayorÃ­a de casos
- CodeLlama 7B es excelente para cÃ³digo sin restricciones
- La UI estÃ¡ optimizada para temas oscuros y reduce el brillo

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible para uso personal y educativo.

---

**GP-Test** - Chat IA Local Sin Sesgo | Optimizado para 25GB RAM | Modelos Sin Restricciones
